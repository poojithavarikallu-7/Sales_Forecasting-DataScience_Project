# -*- coding: utf-8 -*-
"""Sales_Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W3uIx-F_SHbyhjpWmXmPTyD3ieGCRQ8F

# **Sales Forecasting Project**

**Installing catboost module**
"""

pip install catboost

"""**Importing the required libraries & packages**"""

import numpy as np
import pandas as pd
import os
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV
from sklearn.linear_model import LinearRegression,Lasso,Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from scipy.stats import uniform, randint
import pickle
import warnings
warnings.filterwarnings('ignore')

"""**Changing The Default Working Directory Path & Reading the Dataset using Pandas Command**"""

df = pd.read_csv('Train.csv')

"""# **Exploratory Data Analysis (EDA)**
**Getting all the unique value counts from all the columns using lambda function**
"""

df.apply(lambda x : len(x.unique()))

"""**Checking the dataset whether it's having duplicate values or not**"""

df.duplicated().sum()

"""**Checking for the null values of all the columns from the dataset**"""

df.isnull().sum()

"""**Getting the Data types and Non-null count of all the columns from the dataset using .info() statement**"""

df.info()

"""# **Data Cleaning**
**Getting all the columns with "object" data type from the dataset and appending it to the list**
"""

cat_col = []
for x in df.dtypes.index:
    if df.dtypes[x] == 'object':
        cat_col.append(x)
display(cat_col)

"""**Removing the columns Item_Identifier and Outlet_Identifier from the list since the both columns doesn't need any cleaning**"""

cat_col.remove('Item_Identifier')
cat_col.remove('Outlet_Identifier')

"""**Displaying the list after removing certain columns from it to verify**"""

display(cat_col)

"""**Getting the unique value counts of the columns in the list**"""

for col in cat_col:
    print(col,len(df[col].unique()))

"""**Checking the value counts of the columns from the list and displaying it column wise**"""

for col in cat_col:
    print(col)
    print(df[col].value_counts(),'\n')
    print('-'*55)

"""**Getting the null values from the Item_Weight column for the null value treatment process and displaying the Dataset with null values in the Item_Weight column**"""

miss_bool = df['Item_Weight'].isnull()
Item_Weight_Null = df[df['Item_Weight'].isnull()]
display(Item_Weight_Null)

"""**Identifying the unique value counts in Item_Identifier column from the Item_Weight null value dataset**"""

Item_Weight_Null['Item_Identifier'].value_counts()

"""**Getting the mean values of the Item_Weight with respect to Item_Identifier column of the dataset using Pivot Table function**"""

Item_Weight_Mean = df.pivot_table(values = 'Item_Weight', index = 'Item_Identifier')
display(Item_Weight_Mean)

"""**Treating the missing values of the Item_Weight column with the mean values we got above using Pivot Table function and filling it out with respect to Item_Identifier column**"""

for i, item in enumerate(df['Item_Identifier']):
    if miss_bool[i]:
        if item in Item_Weight_Mean:
            df['Item_Weight'][i] = Item_Weight_Mean.loc[item]['Item_Weight']
        else:
            df['Item_Weight'][i] = np.mean(df['Item_Weight'])

"""**After treating the null values in the Item_Weight column, checking for the null value in the column**"""

df['Item_Weight'].isna().sum()

"""**Getting the unique value counts from Outlet_Size column from the dataset**"""

df['Outlet_Size'].value_counts()

"""**Checking out for the null value counts from the Outlet_Size column from the dataset**"""

df['Outlet_Size'].isnull().sum()

"""**Getting the null values from the Outlet_Size column for the null value treatment process and displaying the Dataset with null values in the Outlet_Size column**"""

Outlet_Size_Null = df[df['Outlet_Size'].isna()]
display(Outlet_Size_Null)

"""**Getting the value counts of Outlet_Type from the Outlet_Size null dataset**"""

Outlet_Size_Null['Outlet_Type'].value_counts()

"""**Grouping by Outlet_Type and Outlet_Size with the aggregate function of size of the Outlet_Type column value**s"""

df.groupby(['Outlet_Type','Outlet_Size']).agg({'Outlet_Type':[np.size]})

"""**Getting the mode values of the Outlet_Size with respect to Outlet_Type column of the dataset using Pivot Table function**"""

Outlet_Size_Mode = df.pivot_table(values = 'Outlet_Size', columns = 'Outlet_Type', aggfunc = (lambda x : x.mode()[0]))
display(Outlet_Size_Mode)

"""**Getting the null values of Outlet_Size column from the dataset and treating the null value using mode values of the Outlet_Size with respect to Outlet_Type column**"""

miss_bool = df['Outlet_Size'].isna()
df.loc[miss_bool,'Outlet_Size'] = df.loc[miss_bool,'Outlet_Type'].apply(lambda x : Outlet_Size_Mode[x])

"""**After missing value treatment of the Outlet_Size column, checking for the null values in the column**"""

df['Outlet_Size'].isna().sum()

"""**Checking for the null values of all the columns from the dataset after missing value treatment**"""

df.isna().sum()

"""**Getting the count of Item_Visibility column with value 0**"""

sum(df['Item_Visibility'] == 0)

"""**Filling out the 0 values from the Item_Visibility column with mean values using replace function**"""

df.loc[:,'Item_Visibility'].replace([0],[df['Item_Visibility'].mean()],inplace = True)

"""**Now, again checking out for 0 values in the Item_Visibility column after filling it out to verify any misplacement happened**"""

sum(df['Item_Visibility'] == 0)

"""**Getting the unique value counts from the Item_Fat_Content column**"""

df['Item_Fat_Content'].value_counts()

"""**After seeing the unique value counts from the Item_Fat_Content column, there have been some mistyping occured like the same categories were typed under different names. For further processing, all the mistypings are corrected and named under a single category. Checking out for the value counts of Item_Fat_Content column**"""

df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'LF' : 'Low Fat', 'low fat' : 'Low Fat', 'reg' : 'Regular'})
df['Item_Fat_Content'].value_counts()

"""**Adding new column New_Item_Type to the dataset by getting the first two characters from the Item_Identifier column which represents the category of the item and getting the value counts of the New_Item_Type column**"""

df['New_Item_Type'] = df['Item_Identifier'].apply(lambda x : x[:2])
df['New_Item_Type'].value_counts()

"""**As the New_Item_Type column has values which is subjected to categories for better understanding, replacing the codes with meaningful categorical item name and getting the value counts of New_Item_Type column**"""

df['New_Item_Type'] = df['New_Item_Type'].replace({'FD' : 'Food', 'NC' : 'Non-Consumables', 'DR' : 'Drinks'})
df['New_Item_Type'].value_counts()

"""**Grouping by New_Item_Type and Item_Fat_Content with the aggregate function of size of the Outlet_Type column values**"""

df.groupby(['New_Item_Type','Item_Fat_Content']).agg({'Outlet_Type':[np.size]})

"""**From the above output its clear that Non-Consumable type from New_Item_Type column is mapped to Low Fat category in Item_Fat_Content column. So marking it as Non-Edible in Item_Fat_Content column**"""

df.loc[df['New_Item_Type'] == 'Non-Consumables','Item_Fat_Content'] = 'Non-Edible'
df['Item_Fat_Content'].value_counts()

"""**Getting all the unique value from Outlet_Establishment_Year column from the dataset**"""

df['Outlet_Establishment_Year'].unique()

"""**The Outlet_Establishment_Year column from the dataset has no significance on its own so calculating the years of outlet established until this year and adding it as Outlet_Years column to the dataset**"""

curr_time = datetime.datetime.now()
df['Outlet_Years'] = df['Outlet_Establishment_Year'].apply(lambda x: curr_time.year - x)

"""# **Data Visualization**
**Plotting the Bar Graph with count of Item_Fat_Content and confirm that there are no null values and identify all unique values from the Item_Fat_Content and saving the PNG File**
"""

plt.rcParams['figure.figsize'] = 15,10
plt.style.use('fivethirtyeight')
plot = sns.countplot(x = df['Item_Fat_Content'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Item_Fat_Content')
plt.savefig('Count of Item_Fat_Content.png')
plt.show()

"""**Plotting the Bar Graph with count of Item_Type and confirm that there are no null values and identify all unique values from the Item_Type and saving the PNG File**"""

plot = sns.countplot(x = df['Item_Type'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.xticks(rotation = 90)
plt.title('Count of Item_Type')
plt.savefig('Count of Item_Type.png')
plt.show()

"""**Plotting the Bar Graph with count of Outlet_Establishment_Year and confirm that there are no null values and identify all unique values from the Outlet_Establishment_Year and saving the PNG File**"""

plot = sns.countplot(x = df['Outlet_Establishment_Year'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Outlet_Establishment_Year')
plt.savefig('Count of Outlet_Establishment_Year.png')
plt.show()

"""**Plotting the Bar Graph with count of Outlet_Location_Type and confirm that there are no null values and identify all unique values from the Outlet_Location_Type and saving the PNG File**"""

plot = sns.countplot(x = df['Outlet_Location_Type'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Outlet_Location_Type')
plt.savefig('Count of Outlet_Location_Type.png')
plt.show()

"""**Plotting the Bar Graph with count of Outlet_Size and confirm that there are no null values and identify all unique values from the Outlet_Size and saving the PNG File**"""

plot = sns.countplot(x = df['Outlet_Size'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Outlet_Size')
plt.savefig('Count of Outlet_Size.png')
plt.show()

"""**Plotting the Bar Graph with count of Outlet_Type and confirm that there are no null values and identify all unique values from the Outlet_Type and saving the PNG File**"""

plot = sns.countplot(x = df['Outlet_Type'])
for p in plot.patches:
    plot.annotate(p.get_height(),(p.get_x() + p.get_width() / 2.0,p.get_height()),
                 ha = 'center',va = 'center',xytext = (0,5),textcoords = 'offset points')
plt.title('Count of Outlet_Type')
plt.savefig('Count of Outlet_Type.png')
plt.show()

"""**Visualizing the data distribution of the Item_weight column against the density distribution using Seaborn Distplot and saving the PNG file**"""

sns.distplot(df['Item_Weight'],bins = 20)
plt.title('Distribution of Item_Weight')
plt.savefig('Distribution of Item_Weight.png')
plt.show()

"""**Getting the Correlation Values from all the numeric columns from the dataset using Seaborn Heatmap & saving the PNG File**"""

sns.heatmap(df.select_dtypes(include=np.number).corr(), cmap = 'binary', cbar = True, annot = True, square = True)
plt.title('Correlation Heat Map')
plt.savefig('Correlation Heat Map.png')
plt.show()

"""# **Data Preprocessing**
**Label Encoding the Outlet_Identifier column and adding it as a new column Outlet to the dataset**
"""

le = LabelEncoder()
df['Outlet'] = le.fit_transform(df['Outlet_Identifier'])

"""**Getting the data types of all the columns to find out the "object" data types columns for preprocessing before assigning it to dependent variable and independent variable**"""

df.dtypes

"""**Adding all the necessary column with "object" data types to the list and Label Encoding the columns**"""

cat_col = ['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type','New_Item_Type']
for col in cat_col:
    df[col] = le.fit_transform(df[col])

"""**One Hot Encoding the columns Item_Fat_Content,Outlet_Size,Outlet_Location_Type,Outlet_Type,New_Item_Type using get dummies function**"""

df = pd.get_dummies(df,columns = ['Item_Fat_Content','Outlet_Size','Outlet_Location_Type','Outlet_Type','New_Item_Type'])

"""**Assigning the dependent and independent variable**"""

x = df.drop(['Item_Identifier','Outlet_Identifier','Outlet_Establishment_Year','Item_Outlet_Sales'],axis=1)
y=df['Item_Outlet_Sales']

"""# **Model Fitting**
**Splitting the dependent variable & independent variable into training and test dataset using train test split**
"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 10)

"""**Defining the function for the ML algorithm and fitting it with the passing dependent and independent variable, predicting the dependent variable using algorithm with independent variable. With using cross val score [Cross Validation] process getting the Model Report with cv_score using "neg_mean_squared_error" as scoring and also getting the absolute average mean of cv_score. After that using cross val score [Cross Validation] process, getting the cv_score as R2 score using default scoring parameter and also again getting the mean value of cv_score with default scoring as Average R2 score. End of the function the Accuracy for full data is actually determined using actual R2 score founded between the dependent variable and predicted dependent variable. Atlast getting the coefficient of algorithm with all the columns and plotting the graph using coefficient value of algorithm to show impact of the each column**"""

def train(model, x, y):
    model.fit(x, y)
    pred = model.predict(x)
    cv_score = cross_val_score(model,x,y,scoring = 'neg_mean_squared_error', cv = 10)
    print('Model Report : \n')
    print('Scoring - neg_mean_squared_error')
    print(cv_score,'\n')
    cv_score = np.abs(np.mean(cv_score))
    print('Absolute Average of neg_mean_squared_error : ',cv_score)
    cv_score = cross_val_score(model, x, y, cv = 10)
    print()
    print('R2 Score')
    print(cv_score,'\n')
    cv_score = np.mean(cv_score)
    print('Average R2 Score : ',cv_score,'\n')
    print('Accuracy for Full Data :')
    print('R2 Score : ',r2_score(y,pred),'\n')
    coef = pd.Series(model.coef_, x.columns).sort_values()
    print (coef)
    coef.plot(kind='bar', title="Model Coefficients")
    plt.show()

"""**Fitting the Linear Regression algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = LinearRegression()
train(model,x_train,y_train)

"""**Fitting the Ridge algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = Ridge()
train(model, x_train, y_train)

"""**Fitting the Lasso algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = Lasso()
train(model, x_train, y_train)

"""**Defining the function for the ML algorithm and fitting it with the passing dependent and independent variable, predicting the dependent variable using algorithm with independent variable. With using cross val score [Cross Validation] process getting the Model Report with cv_score using "neg_mean_squared_error" as scoring and also getting the absolute average mean of cv_score. After that using cross val score [Cross Validation] process, getting the cv_score as R2 score using default scoring parameter and also again getting the mean value of cv_score with default scoring as Average R2 score. End of the function the Accuracy for full data is actually determined using actual R2 score founded between the dependent variable and predicted dependent variable. Atlast getting the feature importance of all the columns and plotting the graph using feature importance of algorithm to show impact of the each column**"""

def train(model, x, y):
    model.fit(x, y)
    pred = model.predict(x)
    cv_score = cross_val_score(model,x,y,scoring = 'neg_mean_squared_error', cv = 10)
    print('Model Report : \n')
    print('Scoring - neg_mean_squared_error')
    print(cv_score,'\n')
    cv_score = np.abs(np.mean(cv_score))
    print('Absolute Average of neg_mean_squared_error : ',cv_score)
    cv_score = cross_val_score(model, x, y, cv = 10)
    print()
    print('R2 Score')
    print(cv_score,'\n')
    cv_score = np.mean(cv_score)
    print('Average R2 Score : ',cv_score,'\n')
    print('Accuracy for Full Data :')
    print('R2 Score : ',r2_score(y,pred),'\n')
    coef = pd.Series(model.feature_importances_, x.columns).sort_values(ascending=False)
    coef.plot(kind='bar', title="Feature Importance")
    plt.show()

"""**Fitting the Decision Tree Regressor algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = DecisionTreeRegressor()
train(model, x_train, y_train)

"""**Fitting the Random Forest Regressor algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = RandomForestRegressor()
train(model, x_train, y_train)

"""**Fitting the Extra Trees Regressor algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = ExtraTreesRegressor()
train(model, x_train, y_train)

"""**Fitting the LGBM Regressor algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = LGBMRegressor()
train(model, x_train, y_train)

"""**Fitting the XGB Regressor algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = XGBRegressor()
train(model, x_train, y_train)

"""**Fitting the Cat Boost Regressor algorithm to the model and passing it to the defined function with train dependent and train independent variable and getting the output for the defined function**"""

model = CatBoostRegressor(verbose = 0)
train(model, x_train, y_train)

"""**Passing some of the list of parameters for the Random Forest Regressor Model to run with Randomized Search CV Algorithm**"""

random_grid = {
               'max_features': ['auto', 'sqrt'],
               'max_depth': [int(x) for x in np.linspace(5, 30, num = 6)],
               'min_samples_split':[2, 5, 10, 15, 100],
               'min_samples_leaf': [1, 2, 5, 10]
}

"""**Fitting The Random Forest Regressor Model with the above mentioned parameters in the RandomizedSearchCV Algorithm and displaying the Best Parameters, Best Score and R2 Score between test dependent variable and predicted dependent variable**"""

RF = RandomForestRegressor()
RF = RandomizedSearchCV(estimator = RF, param_distributions = random_grid, scoring = 'neg_mean_squared_error', n_iter =10,
                       verbose = 0, cv =10, random_state = 10, n_jobs = 1)
RF.fit(x_train, y_train)
print('Best Params : ',RF.best_params_,'\n')
print('Best Score : ',RF.best_score_,'\n')
prediction = RF.predict(x_test)
print('R2 Score : ',r2_score(y_test,prediction))

"""**Visualizing the data distribution of the dependent test variable , predicted dependent variable of the Random Forest Regressor Model against the density distribution using Seaborn Distplot**"""

sns.distplot(y_test-prediction)
plt.show()

"""**Passing some of the list of parameters for the LGBM Regressor Model to run with Randomized Search CV Algorithm**"""

params = {
    "learning_rate": uniform(0.03, 0.3),
    "max_depth": randint(2, 6),
    "n_estimators": randint(100, 150),
    "subsample": uniform(0.6, 0.4)
}

"""**Fitting The LGBM Regressor Model with the above mentioned parameters in the RandomizedSearchCV Algorithm and displaying the Best Parameters, Best Score and R2 Score between test dependent variable and predicted dependent variable**


"""

lgb = LGBMRegressor()
lgb = RandomizedSearchCV(estimator = lgb, param_distributions = params, cv = 10, n_iter = 10, verbose = 0,
                        scoring = 'neg_mean_squared_error', n_jobs = 1, random_state = 10)
lgb.fit(x_train,y_train)
print('Best Params : ',lgb.best_params_,'\n')
print('Best Score : ',lgb.best_score_,'\n')
prediction = lgb.predict(x_test)
print('R2 Score : ',r2_score(y_test,prediction))

"""**Visualizing the data distribution of the dependent test variable , predicted dependent variable of the LGBM Regressor Model against the density distribution using Seaborn Distplot**"""

sns.distplot(y_test-prediction)
plt.show()

"""**Passing some of the list of parameters for the XGB Regressor Model to run with Randomized Search CV Algorithm**"""

params = {
    "gamma": uniform(0, 0.5),
    "learning_rate": uniform(0.03, 0.3),
    "max_depth": randint(2, 6),
    "n_estimators": randint(100, 150),
    "subsample": uniform(0.6, 0.4)
}

"""**Fitting The XGB Regressor Model with the above mentioned parameters in the RandomizedSearchCV Algorithm and displaying the Best Parameters, Best Score and R2 Score between test dependent variable and predicted dependent variable**"""

xgb = XGBRegressor()
xgb = RandomizedSearchCV(estimator = xgb, param_distributions = params, cv = 10, n_iter = 10, verbose = 0,
                        scoring = 'neg_mean_squared_error', n_jobs = 1, random_state = 10)
xgb.fit(x_train, y_train)
print('Best Params : ',xgb.best_params_,'\n')
print('Best Score : ',xgb.best_score_,'\n')
prediction = xgb.predict(x_test)
print('R2 Score : ',r2_score(y_test,prediction))

"""**Visualizing the data distribution of the dependent test variable , predicted dependent variable of the XGB Regressor Model against the density distribution using Seaborn Distplot**"""

sns.distplot(y_test-prediction)
plt.show()

"""**Passing some of the list of parameters for the CatBoost Regressor Model to run with Randomized Search CV Algorithm**"""

params = {
    "learning_rate": uniform(0.03, 0.3),
    "max_depth": randint(2, 6),
    "n_estimators": randint(100, 150),
    "subsample": uniform(0.6, 0.4)
}

"""**Fitting The CatBoost Regressor Model with the above mentioned parameters in the RandomizedSearchCV Algorithm and displaying the Best Parameters, Best Score and R2 Score between test dependent variable and predicted dependent variable**"""

cat = CatBoostRegressor(verbose = 0)
cat = RandomizedSearchCV(estimator = cat, param_distributions = params, cv = 10, n_iter = 10, verbose = 0,
                        scoring = 'neg_mean_squared_error', n_jobs = 1, random_state = 10)
cat.fit(x_train,y_train)
print('Best Params : ',cat.best_params_,'\n')
print('Best Score : ',cat.best_score_,'\n')
prediction = cat.predict(x_test)
print('R2 Score : ',r2_score(y_test,prediction))

"""**Visualizing the data distribution of the dependent test variable , predicted dependent variable of the CatBoost Regressor Model against the density distribution using Seaborn Distplot**"""

sns.distplot(y_test-prediction)
plt.show()

"""**Fitting The CatBoost Regressor Model with the best params got from the Randomized SearchCV and predicting the test dependent data to verify the r2 score with the r2 score got from Randomized SearchCV**"""

cat = CatBoostRegressor(learning_rate = 0.08941885942788719, max_depth = 2, n_estimators = 109,
                        subsample = 0.6676443346250142, verbose = 0)
cat.fit(x_train,y_train)
predictions = cat.predict(x_test)
print('R2 score : ',r2_score(y_test,predictions))

"""# **Model Testing**
**Create the pickle file of the model with the highest r2 score with the name Model**
"""

pickle.dump(cat,open('Model.pkl','wb'))

"""**Loading the pickle file and predicting the dependent variable for the whole data and getting the r2 score between the predicted dependent variable and dependent variable**"""

model = pickle.load(open('Model.pkl','rb'))
fpred = model.predict(x)
print('R2 Score of Full Data : ',r2_score(y,fpred))